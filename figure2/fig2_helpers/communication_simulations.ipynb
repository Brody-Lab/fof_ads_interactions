{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(\"color\", plt.cm.Spectral(np.linspace(0,1,8)))\n",
    "plt.rcParams['figure.figsize'] = [4, 3]\n",
    "plt.rcParams[\"font.family\"] = \"Helvetica\"\n",
    "\n",
    "sys.path.insert(1, '../../../figure_code/')\n",
    "from my_imports import *\n",
    "from helpers.phys_helpers import flatten_list, xcorr, savethisfig\n",
    "from helpers.rasters_and_psths import make_psth, get_neural_activity\n",
    "from logisticdecoding import run_logistic_decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nanpad_ragged_sequence(ragged_seq):\n",
    "    \n",
    "    max_length = max(len(subseq) for subseq in ragged_seq)\n",
    "\n",
    "    # Create a new list with padded sequences\n",
    "    padded_seq = [list(subseq) + [np.nan] * (max_length - len(subseq)) for subseq in ragged_seq]\n",
    "\n",
    "    # Convert the list to a NumPy array\n",
    "    array_with_nan_padding = np.array(padded_seq)\n",
    "    \n",
    "    return array_with_nan_padding\n",
    "\n",
    "\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'same') / w\n",
    "\n",
    "\n",
    "def psych_func(x, s, b, g, l):\n",
    "    '''\n",
    "    sigmoid for curve fitting\n",
    "    '''\n",
    "    return g + l * expit(s * (x+b))\n",
    "\n",
    "\n",
    "def make_dataframes(task_data, ns, spikes):\n",
    "\n",
    "    df_trial = pd.DataFrame(task_data)\n",
    "    df_trial = df_trial.rename(columns = {\"choice\": \"pokedR\", \"history_bias\": \"history\", \"T\": \"stim_dur\"})\n",
    "    df_trial[\"clicks_on\"] = np.arange(len(df_trial))\n",
    "    df_trial[\"clicks_off\"] = df_trial[\"clicks_on\"] + df_trial[\"stim_dur\"]\n",
    "    \n",
    "    df_cell = pd.DataFrame()\n",
    "    for i in range(2*ns.N_neurons_per_region):\n",
    "        df_cell.loc[i, 'region'] = \"A\" if i < ns.N_neurons_per_region else \"B\"\n",
    "        df_cell.loc[i, 'spiketime_s'] = ''\n",
    "        spiketime_s = []\n",
    "        for tr in range(len(df_trial)):\n",
    "            this_spikes = np.where(spikes[tr,:,i] > 0)[0]*ns.dt\n",
    "            if this_spikes.size > 0:\n",
    "                spiketime_s.append([np.round(s + df_trial.loc[tr, 'clicks_on'],3) for s in this_spikes])\n",
    "        df_cell['spiketime_s'][i] = flatten_list(spiketime_s)\n",
    "\n",
    "    return df_trial, df_cell\n",
    "\n",
    "\n",
    "def crosscorrelate_DVs(DV, prm, shuffle = False):\n",
    "    \n",
    "    regions = list(DV)\n",
    "    num_tpts = DV[regions[0]].shape[1]\n",
    "    num_trials = DV[regions[0]].shape[0]\n",
    "    DV_crosscoef = np.nan*np.zeros((num_trials, 2*num_tpts-1))\n",
    "    \n",
    "    # set shuffle\n",
    "    if shuffle == True:\n",
    "        l0 = np.random.permutation(num_trials)\n",
    "    else:\n",
    "        l0 = range(num_trials)\n",
    "        \n",
    "    # now cross-correlate\n",
    "    for tr, tr0 in enumerate(l0):\n",
    "        nonnan = ~np.isnan(DV['A'][tr, :])\n",
    "        lags, values = xcorr(\n",
    "            DV['A'][tr, nonnan],\n",
    "            DV['B'][tr0, nonnan],\n",
    "            detrend = False,\n",
    "            scale = 'unbiased'\n",
    "        )\n",
    "        DV_crosscoef[tr, lags + num_tpts -1] = values\n",
    "        \n",
    "    # create summary\n",
    "    DV_dict = dict()\n",
    "    DV_dict['DV_crosscoef'] = DV_crosscoef\n",
    "    DV_dict['lags'] = np.arange(-num_tpts+1, num_tpts) * prm['binsize'] * 1e-3\n",
    "    DV_dict['mean'] = np.nanmean(DV_crosscoef, axis = 0)\n",
    "    DV_dict['std'] = np.nanstd(DV_crosscoef, axis = 0)\n",
    "    idx = np.abs(DV_dict['lags']) < 0.1\n",
    "    peak_idx = np.argmax(np.nanmean(DV_crosscoef[:,idx], axis = 0))\n",
    "    DV_dict['peak'] = DV_dict['lags'][idx][peak_idx]\n",
    "    \n",
    "    return DV_dict\n",
    "\n",
    "\n",
    "def plot_cross_corr_metrics(DV_summary, ns, summary, df_trial, latents):\n",
    "    \n",
    "    fig_DV = plt.figure(constrained_layout = True, figsize = (10,10))\n",
    "    gs = fig_DV.add_gridspec(5,4)\n",
    "    fig_DV.suptitle('DV cross correlation summary', fontsize = 16)\n",
    "            \n",
    "    regions = list(DV_summary['mdl_coefs'])\n",
    "    colors = ['b', 'g']\n",
    "    # plot inferred DV axis compared to emissions\n",
    "    for r, reg in enumerate(regions):\n",
    "        ax0 = fig_DV.add_subplot(gs[0, r])\n",
    "        this_modelcoef = DV_summary['mdl_coefs'][reg].T\n",
    "        this_emissions = ns.C[r*nn:(r+1)*nn,r*ns.N_latents_per_region]\n",
    "        ax0.plot(\n",
    "            this_modelcoef/np.linalg.norm(this_modelcoef),\n",
    "            label = 'DV axis',\n",
    "            c = 'k')\n",
    "        ax0.plot(\n",
    "            this_emissions/np.linalg.norm(this_emissions), \n",
    "            c= 'r', \n",
    "            label = 'weights')\n",
    "        this_corr = np.round(np.corrcoef(this_modelcoef.T, this_emissions)[0,1],2)\n",
    "        ax0.legend(frameon = False)\n",
    "        ax0.set_xlabel('Neuron number')\n",
    "        ax0.set_title('REG: {} | Corr: {}'.format(reg, this_corr))\n",
    "        \n",
    "    # plot decoding accuracy\n",
    "    ax0 = fig_DV.add_subplot(gs[0,2])\n",
    "    for r, reg in enumerate(regions):\n",
    "        ax0.plot(\n",
    "            summary[reg]['accuracy'], \n",
    "            label = \"reg\" + reg,\n",
    "            color = colors[r])\n",
    "    ax0.legend()\n",
    "    ax0.set_ylim([0.5, 1.0])\n",
    "    ax0.set_title(\"Decoding accuracy\")\n",
    "        \n",
    "    # next plot a couple of examples \n",
    "    tr = np.random.choice(nt)\n",
    "    ax0 = fig_DV.add_subplot(gs[0,3])\n",
    "    for r, reg in enumerate(regions):\n",
    "        ax0.plot(\n",
    "            DV_summary['DV'][reg][tr,:], \n",
    "            label = \"reg\" + reg,\n",
    "            color = colors[r])\n",
    "    ax0.legend(frameon = False)\n",
    "    ax0.set_title(\"trial: {}\".format(tr))\n",
    "    ax0.set_ylabel('inferred DV')    \n",
    "    ax0.set_ylim([-10,10])\n",
    "\n",
    "\n",
    "    # plot DVs averaged over gammas from the two regions\n",
    "    for r, reg in enumerate(regions):\n",
    "            \n",
    "        ax = fig_DV.add_subplot(gs[1+r, 0:2])\n",
    "        for u in np.unique(task_data['gamma']):\n",
    "            rtrials = np.where(np.array(task_data['gamma']) == u)[0]\n",
    "            ax.plot(np.nanmean(DV_summary['DV'][reg][rtrials, :], axis = 0), label = u)\n",
    "        # ax3.legend(ncol = 2, frameon = False)\n",
    "        ax.set_title('REG: {} '.format(reg))\n",
    "        ax.set_ylabel('Mean DV for gammas')\n",
    "        ax.set_ylim([-5,5])\n",
    "\n",
    "    # plot DV crosscorrelation\n",
    "    def plot_cc(this_cc, color, ax, label):\n",
    "        mn = np.nanmean(this_cc['DV_crosscoef'], axis = 0)\n",
    "        std = np.nanstd(this_cc['DV_crosscoef'], axis = 0)/np.sqrt(this_cc['DV_crosscoef'].shape[0])\n",
    "        ax.plot(this_cc['lags'], mn, c= color, label = label)\n",
    "        ax.fill_between(this_cc['lags'],  \n",
    "                    mn - std, \n",
    "                    mn + std, \n",
    "                    alpha = 0.5, \n",
    "                    color = color,\n",
    "                    edgecolor = None)\n",
    "\n",
    "    # now plot cross-correlation \n",
    "    ax = fig_DV.add_subplot(gs[3:5,0:2])\n",
    "    plot_cc(DV_summary['DV_cc'], 'k', ax, 'DATA')\n",
    "    ax.axvline(DV_summary['DV_cc']['peak'], c = 'k')\n",
    "    plot_cc(DV_summary['DV_cc_shuff'], 'r', ax, 'shuffle')\n",
    "    ax.legend(frameon = False)\n",
    "    ax.axvline(0, ls = ':')\n",
    "    ax.axhline(0, c = 'k',ls = ':')\n",
    "    ax.set_xlim([-0.4, 0.4])\n",
    "    ax.set_xlabel('Lags [s]')\n",
    "    ax.set_ylabel('Decision variable \\n correlation (A, B)')\n",
    "    # ax.set_ylim([-0.1, 0.2])\n",
    "    for r, reg in enumerate(regions):\n",
    "        ax.text(-0.1 + r*0.2,  \n",
    "            ax.get_ylim()[1], \n",
    "            reg + ' leads', \n",
    "            fontsize=10, \n",
    "            va='center', \n",
    "            ha='center', \n",
    "            backgroundcolor='w')\n",
    "\n",
    "\n",
    "    # plot accumulation latents against each other\n",
    "    ax0 = fig_DV.add_subplot(gs[1:3,2:4])\n",
    "    for pR in np.unique(df_trial['pokedR']):\n",
    "        idx = df_trial['pokedR'] == pR\n",
    "        ax0.scatter(\n",
    "            np.ravel(latents[idx,:,0]),\n",
    "            np.ravel(latents[idx,:,3]), \n",
    "            marker = '.',\n",
    "            alpha = 0.5,\n",
    "            label = 'choice = {}'.format(pR))\n",
    "    ax0.legend(frameon = False)\n",
    "    ax0.set_xlabel('Accumulation latent reg A')\n",
    "    ax0.set_ylabel('Accumulation latent reg B')\n",
    "        \n",
    "            \n",
    "    # plot inferred DVs against each other\n",
    "    ax0 = fig_DV.add_subplot(gs[3:5,2:4])\n",
    "    for pR in np.unique(df_trial['pokedR']):\n",
    "        idx = df_trial['pokedR'] == pR\n",
    "        nonnan = ~np.isnan(DV_summary['DV']['A'])[idx,:]\n",
    "        plt.scatter(\n",
    "            np.ravel(DV_summary['DV']['A'][np.where(nonnan == 1)]),\n",
    "            np.ravel(DV_summary['DV']['B'][np.where(nonnan == 1)]), \n",
    "            marker = '.',\n",
    "            alpha = 0.5,\n",
    "            label = 'choice = {}'.format(pR))\n",
    "    ax0.legend(frameon = False)\n",
    "    ax0.set_xlabel('DV reg A')\n",
    "    ax0.set_ylabel('DV reg B')\n",
    "    ax0.set_xlim([-8,8])\n",
    "    ax0.set_ylim([-8,8])\n",
    "        \n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class to generate stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoissonClicks():\n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        \n",
    "        self.dt = params.get('dt', 0.01)\n",
    "        self.N_batch = params.get('N_batch', 300)\n",
    "        self.T = params.get('T', [0.2,1.0])\n",
    "        self.gamma_list = params.get('gamma_list', np.linspace(-3.5, 3.5, 8))\n",
    "        self.history_bias = params.get('history_bias', True)\n",
    "        self.total_rate = params.get('total_rate', 40)\n",
    "        \n",
    "        \n",
    "    def generate_trial_params(self, batch, trial):\n",
    "        \n",
    "        prm = dict()\n",
    "        prm['gamma'] = np.random.choice(self.gamma_list)\n",
    "        \n",
    "        if self.history_bias == True:\n",
    "            prm['history_bias'] = np.random.choice([-1,1])\n",
    "        else:\n",
    "            prm['history_bias'] = 0\n",
    "            \n",
    "        prm['T'] = self.T[0] + np.random.rand()*(self.T[1] - self.T[0])\n",
    "            \n",
    "        return prm\n",
    "    \n",
    "    \n",
    "    def generate_trial(self, prm):\n",
    "        \n",
    "        # generate clicks\n",
    "        rate_r = self.total_rate * np.exp(prm['gamma'])/(1+np.exp(prm['gamma'])) + 1e-16\n",
    "        rate_l = self.total_rate - rate_r\n",
    "        \n",
    "        # click times\n",
    "        click_time_r = np.cumsum(np.random.exponential(1/rate_r, 100))\n",
    "        click_time_l = np.cumsum(np.random.exponential(1/rate_l, 100))\n",
    "        \n",
    "        # binned outputs with dimensions     \n",
    "        binned_r = np.histogram(click_time_r, np.arange(0., prm['T'] + self.dt, self.dt))[0]\n",
    "        binned_l = np.histogram(click_time_l, np.arange(0., prm['T'] + self.dt, self.dt))[0]\n",
    "        \n",
    "        prm['Δclicks'] = np.sum(binned_r) - np.sum(binned_l)\n",
    "        \n",
    "        return binned_r - binned_l, prm\n",
    "    \n",
    "    \n",
    "    def batch_generator(self):\n",
    "        \n",
    "        batch = 1\n",
    "        while batch > 0:\n",
    "            \n",
    "            click_data = []\n",
    "            task_data = dict()\n",
    "            \n",
    "            for trial in range(self.N_batch):\n",
    "                p = self.generate_trial_params(batch, trial)\n",
    "                out, p = self.generate_trial(p)\n",
    "                click_data.append(out)\n",
    "                \n",
    "                for key in list(p):\n",
    "                    if trial == 0:\n",
    "                        task_data[key] = []\n",
    "                    task_data[key].append(p[key])\n",
    "                    \n",
    "            batch += 1\n",
    "            \n",
    "            yield nanpad_ragged_sequence(click_data), task_data\n",
    "            \n",
    "    def get_trial_batch(self):\n",
    "        \n",
    "        return next(self.batch_generator())\n",
    "    \n",
    "    \n",
    "    def get_params(self):\n",
    "        \n",
    "        return self.__dict__\n",
    "                \n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural simulator class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralSimulator():\n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        \n",
    "        self.dt = params.get('dt', 0.01)\n",
    "        self.N_neurons_per_region = params.get('N_neurons_per_region', 50)\n",
    "        self.bound = params.get('bound', 8)\n",
    "        self.leak = params.get('leak', 0.8)\n",
    "        self.N_latents_per_region = params.get('N_latents_per_region', 3)\n",
    "        self.interaction_type = params.get('interaction_type', 'feedforward')\n",
    "        self.history_bias = params.get('history_bias', True)\n",
    "        self.sigmas_init = params.get('sigmas_init', 2.)\n",
    "        self.sigmas = params.get('sigmas', 40)\n",
    "        self.ff_delay = params.get('ff_delay', 1) # in dt units\n",
    "        \n",
    "        num_neurons = 2*self.N_neurons_per_region\n",
    "        num_latents = 2*self.N_latents_per_region\n",
    "        \n",
    "        # dynamics matrix\n",
    "        self.A = np.zeros((2, num_latents, num_latents))\n",
    "        \n",
    "        # dynamics matrix with delay\n",
    "        self.A_delay = np.zeros((2, num_latents, num_latents))\n",
    "        \n",
    "        # inputs matrix\n",
    "        if self.history_bias == True:\n",
    "            self.B = np.zeros((2, num_latents, 2))\n",
    "            assert self.N_latents_per_region >= 2, 'Code is not setup to deal with fewer dims'\n",
    "        else:\n",
    "            self.B = np.zeros((2, num_latents))\n",
    "            assert self.N_latents_per_region >= 1, 'Code is not setup to deal with fewer dims'\n",
    "            \n",
    "        # emissions matrix\n",
    "        self.C = np.zeros((num_neurons, num_latents))\n",
    "        \n",
    "        # emissions bias\n",
    "        self.b = 0.5 + np.random.normal(size = (num_neurons, 1))\n",
    "        \n",
    "        # noise multiplier for bound hitting state\n",
    "        self.noise_mul = np.ones((2, num_latents))\n",
    "        \n",
    "        self.noise_mul[0, self.N_latents_per_region] = 0.\n",
    "        self.noise_mul[1,0] = 0.\n",
    "        self.noise_mul[1, self.N_latents_per_region] = 0.\n",
    "        \n",
    "        # map dynamics matrix based on the interaction type\n",
    "        interaction_dict = {\"feedforward\": self.set_feedforward_params,\n",
    "                            \"distributed\": self.set_distributed_params,\n",
    "                            \"recurrent\": self.set_recurrent_params}\n",
    "        interaction_dict[self.interaction_type]()\n",
    "        \n",
    "        if \"dynamics_matrix\" in params.keys():\n",
    "            print(\"Dynamics matrix specified. These will be given precedence over default settings for the interaction type\")\n",
    "            assert np.shape(self.A) == np.shape(params['dynamics_matrix']), 'Specified dynamics matrix is incompatible'\n",
    "            self.A = params.get('dynamics_matrix')\n",
    "            \n",
    "        if \"input_matrix\" in params.keys():\n",
    "            print(\"Input matrix specified. These will be given precedence over default settings for the interaction type\")    \n",
    "            assert np.shape(self.B) == np.shape(params['input_matrix']), 'Specified input matrix is incompatible'\n",
    "            self.B = params.get('input_matrix')\n",
    "            \n",
    "        if \"emissions_matrix\" in params.keys():\n",
    "            print(\"Emissions matrix specified. These will be given precedence over default settings for the interaction type\")\n",
    "            assert np.shape(self.C) == np.shape(params['emissions_matrix']), 'Specified emissions matrix is incompatible'\n",
    "            self.C = params.get('emissions_matrix')\n",
    "        \n",
    "        \n",
    "    def set_feedforward_params(self):\n",
    "        \n",
    "        N_neurons = self.N_neurons_per_region\n",
    "        N_latents = self.N_latents_per_region\n",
    "        \n",
    "        # set up autoregressive terms\n",
    "        np.fill_diagonal(self.A[0], 0.97 + 0.02*np.random.rand(2*N_latents))\n",
    "        \n",
    "        # set up the accumulation dimension with a small trial history input\n",
    "        self.A[0,0,0] = self.leak\n",
    "        self.A[0,0,1] = 0.1\n",
    "        \n",
    "        # no accumulation in latents of other region, it only inherits - no additional noise\n",
    "        self.A[0, N_latents, N_latents] = 0\n",
    "        self.A_delay[0, N_latents, 0] = 1.\n",
    "        self.A_delay[1, N_latents, 0] = 1.\n",
    "        # self.noise_mul[0, self.N_latents_per_region] = 0.\n",
    "        \n",
    "        # copy the same dynamics when bound is reached, but with no accumulation anymore\n",
    "        self.A[1] = self.A[0]\n",
    "        self.A[1,0,0] = 1. # keep accumulation dimensions where they are\n",
    "        self.A[1,0,1] = 0. # no history input\n",
    "        \n",
    "        # setup inputs\n",
    "        self.B[0,0,0] = 1\n",
    "        self.B[0, N_latents, 0] = 1\n",
    "        \n",
    "        if self.history_bias == True:\n",
    "            self.B[0,1,1] = 1\n",
    "            self.B[0,N_latents+1, 1] = 1\n",
    "            \n",
    "        self.C[:N_neurons, :N_latents] = 8. * np.random.normal(size = (N_neurons, N_latents))\n",
    "        self.C[N_neurons:, N_latents:] = 8. * np.random.normal(size = (N_neurons, N_latents))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def set_distributed_params(self):\n",
    "        pass\n",
    "    \n",
    "    def set_recurrent_params(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def simulate_trial(self, trial, clicks, task_data):\n",
    "        \n",
    "        st = 0\n",
    "        noise_var = np.sqrt(self.dt*self.sigmas)\n",
    "        stim_length = sum(~np.isnan(clicks))\n",
    "        \n",
    "        acc_idx = self.get_accumulator_latent_index()\n",
    "        hist_idx = self.get_history_latent_index()\n",
    "        AR_idx = self.get_AR_latent_index()\n",
    "\n",
    "        for t in range(len(clicks)):\n",
    "            \n",
    "            if np.isnan(clicks[t]):\n",
    "                self.inputs[0] = 0.\n",
    "            elif clicks[t] == 0:\n",
    "                self.inputs[0] = 0.\n",
    "            else:\n",
    "                self.inputs[0] = clicks[t] + np.random.normal()*noise_var\n",
    "            \n",
    "            if t == 0:\n",
    "                self.z[trial,0,:] = np.sqrt(self.dt) * self.sigmas_init * np.random.randn(2*self.N_latents_per_region)\n",
    "                if self.history_bias == True:\n",
    "                    self.inputs[1] = task_data['history_bias'][trial] + np.random.normal()*noise_var\n",
    "                self.z[trial,t,:] += np.matmul(self.B[st], self.inputs)\n",
    "                \n",
    "            else:\n",
    "                self.inputs[1] = 0.\n",
    "                self.z[trial, t, :] = np.matmul(self.A[st], self.z[trial, t-1, :]) + \\\n",
    "                        np.matmul(self.B[st], self.inputs) + \\\n",
    "                            np.random.normal(\n",
    "                                loc = 0.,\n",
    "                                scale = noise_var * self.noise_mul[st],\n",
    "                                size = 2*self.N_latents_per_region)\n",
    "                                \n",
    "                if (t - self.ff_delay) >= 0:\n",
    "                    self.z[trial, t, :] += np.matmul(self.A_delay[st], self.z[trial, t-self.ff_delay, :]) \n",
    "                \n",
    "                # if either of the regions reach bound dynamics switch\n",
    "                if np.abs(self.z[trial, t, 0]) > self.bound:\n",
    "                    st = 1\n",
    "                elif np.abs(self.z[trial, t, self.N_latents_per_region] > self.bound):\n",
    "                    st = 1\n",
    "                    \n",
    "        # assign choice for this trial based on the accumulator\n",
    "        task_data['choice'].append(np.sign(self.z[trial, stim_length-1, 0]) > 0)\n",
    "        \n",
    "            \n",
    "        # normalize so that all latents are about the same value\n",
    "        self.z[trial,:,acc_idx[0]] /= self.bound\n",
    "        self.z[trial,:,acc_idx[1]] /= self.bound\n",
    "        self.z[trial,:,hist_idx[0]] /= 1.\n",
    "        self.z[trial,:,hist_idx[1]] /= 1.\n",
    "        self.z[trial,:,AR_idx[0]] /= 1.\n",
    "        self.z[trial,:,AR_idx[1]] /= 1.\n",
    "        \n",
    "        # compute spike rates and sample spikes for the neural population\n",
    "        spike_rate = np.matmul(self.C, self.z[trial, :, :].T) + self.b\n",
    "        self.spike_rate[trial, :, :] = self.dt * np.maximum(spike_rate,0).T\n",
    "\n",
    "        return task_data                    \n",
    "                                            \n",
    "        \n",
    "    def simulate_trials(self, click_data, task_data):\n",
    "        \n",
    "        ntrials = click_data.shape[0]\n",
    "        num_tpts = click_data.shape[1]\n",
    "        task_data['choice'] = []\n",
    "        \n",
    "        # initialize the matrix for storing evolution of latent variables, spike rates\n",
    "        self.z = np.nan * np.zeros((ntrials, num_tpts, 2*self.N_latents_per_region))\n",
    "        self.spike_rate = np.nan * np.zeros((ntrials, num_tpts, 2*self.N_neurons_per_region))\n",
    "        self.inputs = np.zeros(self.B.shape[2])\n",
    "        self.clicks = click_data\n",
    "        \n",
    "        for trial in range(ntrials):\n",
    "            task_data = self.simulate_trial(trial, click_data[trial,:], task_data)\n",
    "            \n",
    "        self.sample_spikes()\n",
    "        \n",
    "        return self.z, self.spikes, self.spike_rate, task_data\n",
    "    \n",
    "    \n",
    "    def sample_spikes(self, spike_rate_dt = None):\n",
    "        \n",
    "        if spike_rate_dt is None:\n",
    "            spike_rate_dt = self.spike_rate \n",
    "            \n",
    "        self.spikes = np.nan * np.zeros(np.shape(spike_rate_dt))\n",
    "        nanmask = np.isnan(spike_rate_dt)\n",
    "        spike_rate_dt[nanmask] = 1.\n",
    "        \n",
    "        spikes = np.random.poisson(lam = np.ravel(spike_rate_dt[:,:,:]))\n",
    "        self.spikes = spikes.reshape(spike_rate_dt.shape[0], spike_rate_dt.shape[1], spike_rate_dt.shape[2]).astype(float)\n",
    "        \n",
    "        self.spikes[nanmask] = np.nan\n",
    "        spike_rate_dt[nanmask] = np.nan\n",
    "        self.spike_rate[nanmask] = np.nan        \n",
    "    \n",
    "    def get_params(self):\n",
    "        \n",
    "        return self.__dict__\n",
    "    \n",
    "    \n",
    "    def get_latents_labels(self):\n",
    "        \n",
    "        # computing labels of latents dimension based on their number\n",
    "        xlabel_dyn = []\n",
    "        for i in range(2*self.N_latents_per_region):\n",
    "            if i < self.N_latents_per_region:\n",
    "                ap = 'A'\n",
    "                val = 0\n",
    "            else:\n",
    "                ap = 'B'\n",
    "                val = self.N_latents_per_region\n",
    "                \n",
    "            if (i-val) == 0:\n",
    "                xlabel_dyn.append('Acc ' + ap)\n",
    "            elif (i-val) == 1:\n",
    "                xlabel_dyn.append('Hist ' + ap)\n",
    "            else:\n",
    "                xlabel_dyn.append('AR ' + ap)\n",
    "                \n",
    "        return xlabel_dyn\n",
    "    \n",
    "    \n",
    "    def get_accumulator_latent_index(self):\n",
    "        return [0, self.N_latents_per_region]\n",
    "    \n",
    "    def get_history_latent_index(self):\n",
    "        return [1 , self.N_latents_per_region + 1]\n",
    "    \n",
    "    def get_AR_latent_index(self):\n",
    "        num_latents = self.N_latents_per_region\n",
    "        return flatten_list(\n",
    "            [np.arange(2, num_latents), \n",
    "            np.arange(num_latents + 2, 2*num_latents)])\n",
    "    \n",
    "\n",
    "    def plot_dynamics_params(self):\n",
    "        \n",
    "        fig_dyn = plt.figure(constrained_layout = True, figsize = (12,8))\n",
    "        gs = fig_dyn.add_gridspec(5,6)\n",
    "        fig_dyn.suptitle('Simulation parameters', fontsize = 16)\n",
    "        \n",
    "        xlabel_dyn = self.get_latents_labels()\n",
    "                \n",
    "        # dynamics matrix during accumulation\n",
    "        ax0 = fig_dyn.add_subplot(gs[0:2, :2])\n",
    "        ax0.matshow(self.A[0], cmap = 'Blues', vmin = 0., vmax = 1.)\n",
    "        ax0.set_title('Accumulation: dynamics')\n",
    "        ax0.set_yticks(ticks = range(2*self.N_latents_per_region), labels = xlabel_dyn)\n",
    "        ax0.set_xticks(ticks = range(2*self.N_latents_per_region), labels = xlabel_dyn, rotation = 90)\n",
    "        \n",
    "        # input matrix during accumulation\n",
    "        ax1 = fig_dyn.add_subplot(gs[0:2, 2])\n",
    "        ax1.matshow(self.B[0], cmap = 'Blues', vmin = 0., vmax = 1.)\n",
    "        ax1.set_title('Input')\n",
    "        ax1.set_xticks(ticks = [0.5, 2.5], labels = {'δR-δL', 'history'}, rotation = 45)\n",
    "        ax1.set_ylabel('Latents')\n",
    "        \n",
    "        # dynamics matrix once bound has been hit\n",
    "        ax00 = fig_dyn.add_subplot(gs[0:2, 3:5])\n",
    "        ax00.matshow(self.A[1], cmap = 'Blues', vmin = 0., vmax = 1.)\n",
    "        ax00.set_title('Bound hit - dynamics')\n",
    "        ax00.set_yticks(ticks = range(2*self.N_latents_per_region), labels = xlabel_dyn)\n",
    "        ax00.set_xticks(ticks = range(2*self.N_latents_per_region), labels = xlabel_dyn, rotation = 90)\n",
    "        \n",
    "        # input matrix once bound has been hit\n",
    "        ax11 = fig_dyn.add_subplot(gs[0:2,5])\n",
    "        ax11.matshow(self.B[1], cmap = 'Blues', vmin = 0., vmax = 1.)\n",
    "        ax11.set_title('Input')\n",
    "        ax11.set_xticks(ticks = [0.5, 2.5], labels = {'δR-δL', 'history'}, rotation = 45)\n",
    "        ax11.set_xlabel('Latents')\n",
    "        \n",
    "        # emissions matrix\n",
    "        ax2 = fig_dyn.add_subplot(gs[3,:])\n",
    "        ax2.matshow(self.C.T, vmin = -2, vmax = 2, cmap = 'RdBu')\n",
    "        ax2.set_xlabel('Neuron')\n",
    "        ax2.set_ylabel('Latents')\n",
    "        ax2.set_title('Emissions matrix')\n",
    "        \n",
    "        # emissions bias\n",
    "        ax3 = fig_dyn.add_subplot(gs[4,:])\n",
    "        ax3.bar(x = range(2*self.N_neurons_per_region), height = self.b.squeeze(), color = 'k')\n",
    "        ax3.set_title('Emission bias')\n",
    "        ax3.set_xlabel('Neuron')\n",
    "        \n",
    "        sns.despine()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def plot_activity_portrait(self, task_data = None):\n",
    "        \n",
    "        if (hasattr(self, 'spikes') == False) | (task_data == None):\n",
    "            raise Exception('No activity simulated, first run some trials with ns.simulate trials and provide task_data')\n",
    "\n",
    "        fig_act = plt.figure(constrained_layout = True, figsize = (9,12))\n",
    "        gs = fig_act.add_gridspec(7,3)\n",
    "        fig_act.suptitle('Activity portrait', fontsize = 16)\n",
    "        \n",
    "        \n",
    "        \n",
    "        def plot_trial(ax_num):\n",
    "            \n",
    "            labels = self.get_latents_labels()\n",
    "        \n",
    "            this_trial = np.random.choice(len(task_data['gamma']))\n",
    "            # plot an example trial (click trains)\n",
    "            ax3 = fig_act.add_subplot(gs[ax_num,0])\n",
    "            idx = np.ravel(np.argwhere(self.clicks[this_trial,:] != 0))\n",
    "            ax3.scatter(idx*self.dt, self.clicks[this_trial,idx], marker='|', c= 'k')\n",
    "            ax3.set_ylim([-3,3])\n",
    "            ax3.set_xlabel('Time in sec')\n",
    "            ax3.set_ylabel('click difference')\n",
    "            ax3.set_title('Gamma: {}'.format(task_data['gamma'][this_trial]))\n",
    "            \n",
    "            \n",
    "            # plot an example trial (latents)\n",
    "            ax4 = fig_act.add_subplot(gs[ax_num,1])\n",
    "            len_trial = sum(~np.isnan(self.clicks[this_trial, :]))\n",
    "            palette = iter(sns.color_palette(\"Blues\", 5, desat = 1.))\n",
    "            for i in range(self.N_latents_per_region):\n",
    "                color = next(palette)\n",
    "                ax4.plot(np.arange(len_trial)*self.dt, self.z[this_trial, :len_trial, i], c = color, label = labels[i])\n",
    "            palette = iter(sns.color_palette(\"Reds\", 5, desat = 1.))\n",
    "            for i in range(self.N_latents_per_region, 2*self.N_latents_per_region):\n",
    "                color = next(palette)\n",
    "                ax4.plot(np.arange(len_trial)*self.dt, self.z[this_trial, :len_trial, i], c = color, label = labels[i])\n",
    "            ax4.set_xlabel('Time in sec')\n",
    "            ax4.set_ylabel('latent activity [au]')\n",
    "            # ax4.legend(frameon = False)\n",
    "            \n",
    "            # plot an example trial (firing rates)\n",
    "            ax5 = fig_act.add_subplot(gs[ax_num,2])\n",
    "            ax5.imshow(self.spike_rate[this_trial, :len_trial,:].T, interpolation=None)  \n",
    "            ax5.set_aspect('auto')\n",
    "            \n",
    "            \n",
    "            \n",
    "        def plot_latents(ax_num, latent_labels):\n",
    "            \n",
    "            latent_a = np.argwhere([a == latent_labels[0] for a in self.get_latents_labels()])[0][0] \n",
    "            latent_b = np.argwhere([a == latent_labels[1] for a in self.get_latents_labels()])[0][0] \n",
    "        \n",
    "            ax3 = fig_act.add_subplot(gs[ax_num, 0])\n",
    "            ax4 = fig_act.add_subplot(gs[ax_num, 1])\n",
    "            for u in np.unique(task_data['gamma']):\n",
    "                rtrials = np.where(np.array(task_data['gamma']) == u)[0]\n",
    "                ax3.plot(np.mean(self.z[rtrials, :, latent_a], axis = 0), label = u)\n",
    "                ax4.plot(np.mean(self.z[rtrials, :, latent_b], axis = 0), label = u)\n",
    "                # ax3.legend(ncol = 2, frameon = False)\n",
    "            ax3.set_title(latent_labels[0])\n",
    "            ax4.set_title(latent_labels[1])\n",
    "            # ax3.legend(frameon = False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # psychometric curve\n",
    "        ax0 = fig_act.add_subplot(gs[0,0])\n",
    "        # fit a psychometric curve\n",
    "        try:\n",
    "            popt, pcov = curve_fit(\n",
    "                psych_func, \n",
    "                np.array(task_data['Δclicks']).astype('float32'),\n",
    "                np.array(task_data['choice']).astype('float32'),\n",
    "                maxfev = 50000)\n",
    "            psych = psych_func(np.linspace(-40,40, 81), *popt)\n",
    "            ax0.plot(np.linspace(-40,40, 81), psych)\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred during curve fitting:\", e)    \n",
    "        # actual data\n",
    "        df = pd.DataFrame(task_data)\n",
    "        Δcuts, bins = pd.cut(task_data['Δclicks'],bins = 20, retbins = True)\n",
    "        bin_centers = bins[:-1] + np.diff(bins)[0]/2\n",
    "        mean_values = df.groupby(Δcuts)['choice'].mean()\n",
    "        ax0.scatter(bin_centers, mean_values, c = 'k', s = 1)\n",
    "        ax0.set_xlabel('#R - #L')\n",
    "        ax0.set_ylabel('Fraction chose right')\n",
    "        ax0.set_ylim([-0.05,1.05])\n",
    "        \n",
    "        # distribution of firing rates\n",
    "        ax1 = fig_act.add_subplot(gs[0,1])\n",
    "        ax1.hist(np.nanmean(self.spike_rate[:,:,:self.N_neurons_per_region], axis = (0,1))/self.dt, label = 'A', alpha = 0.5, color = 'b')\n",
    "        ax1.hist(np.nanmean(self.spike_rate[:,:,self.N_neurons_per_region:], axis = (0,1))/self.dt, label = 'B', alpha = 0.5, color = 'r')\n",
    "        ax1.legend(frameon = 'false')\n",
    "        ax1.set_ylabel('Mean spks/s')\n",
    "        ax1.set_xlabel('Neuron Count')\n",
    "        \n",
    "            \n",
    "        plot_trial(ax_num = 1)\n",
    "        plot_trial(ax_num = 2)\n",
    "        plot_trial(ax_num = 3)        \n",
    "        plot_latents(ax_num = 4, latent_labels=['Acc A', 'Acc B'])\n",
    "        plot_latents(ax_num = 5, latent_labels=['Hist A', 'Hist B'])\n",
    "        plot_latents(ax_num = 6, latent_labels=['AR A', 'AR B'])\n",
    "        \n",
    "        sns.despine()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['dt'] = 0.001\n",
    "params['history_bias'] = True\n",
    "params['bound'] = 8.\n",
    "params['leak'] = 0.99\n",
    "params['N_neurons_per_region'] = 40\n",
    "params['T'] = [0.2, 1.0]\n",
    "params['N_batch'] = 300\n",
    "params['ff_delay'] = 10\n",
    "\n",
    "pc_task = PoissonClicks(**params)\n",
    "click_data, task_data = pc_task.get_trial_batch()\n",
    "\n",
    "ns = NeuralSimulator(**params)\n",
    "latents, spikes, spike_rate_dt, task_data = ns.simulate_trials(click_data, task_data)\n",
    "\n",
    "df_trial, df_cell = make_dataframes(task_data, ns, spikes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns.plot_dynamics_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns.plot_activity_portrait(task_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellnum = -1\n",
    "cellnum += 1\n",
    "\n",
    "fr = make_psth(\n",
    "    df_cell.iloc[cellnum]['spiketime_s'],\n",
    "    df_trial,\n",
    "    align_to = 'clicks_on',\n",
    "    window = [0, 1000],\n",
    "    binsize = 1,\n",
    "    pre_mask = None,\n",
    "    post_mask = None,\n",
    "    filter_w = 50,\n",
    "    filter_type = 'gaussian',\n",
    "    split_by = \"history\",\n",
    "    plot = True)\n",
    "\n",
    "fr['axs'].set_title(\"{}\".format(np.round(ns.C[cellnum, :],2)))\n",
    "print(ns.get_latents_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "p = dict()\n",
    "p['regions'] = ['A', 'B']\n",
    "p['cols'] = SPEC.COLS\n",
    "p['fr_thresh'] = 1.0 # firing rate threshold for including neurons\n",
    "p['stim_thresh'] = 0.0 # stimulus duration threshold for including trials\n",
    "p['align_to'] = ['clicks_on', 'clicks_on']\n",
    "p['align_name'] = ['clickson_masked', 'clickson_unmasked']\n",
    "p['pre_mask'] = [None, None]\n",
    "p['post_mask'] = ['clicks_off', None]\n",
    "p['start_time'] = [-100, -100]\n",
    "p['end_time'] = [1100, 1500] # in ms\n",
    "p['binsize'] = 50 # in ms\n",
    "p['filter_type'] = 'gaussian'\n",
    "p['filter_w'] = 75 # in ms\n",
    "p['Cs'] = np.logspace(-7,3,200)  # cross-validation parameter\n",
    "p['nfolds'] = 10  # number of folds for cross-validation\n",
    "p['n_repeats'] = 1 # number of repeats for cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_variable = 'pokedR'\n",
    "\n",
    "target = np.array(df_trial[decoded_variable], dtype = float)\n",
    "\n",
    "summary = dict()\n",
    "for r, reg in enumerate(p['regions']):\n",
    "    \n",
    "    X, ntpts_per_trial = get_neural_activity(df_cell, df_trial, reg, p, 0)\n",
    "    summary[reg] = run_logistic_decoding(\n",
    "        X,\n",
    "        target,\n",
    "        ntpts_per_trial,\n",
    "        p\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(summary['A']['accuracy'])\n",
    "plt.plot(summary['B']['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(summary['B']['mdl_coefs'].T/max(summary['B']['mdl_coefs'].T));\n",
    "plt.plot(ns.C[40:,3]/max(ns.C[40:,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = dict()\n",
    "p['regions'] = ['A', 'B']\n",
    "p['cols'] = SPEC.COLS\n",
    "p['fr_thresh'] = 1.0 # firing rate threshold for including neurons\n",
    "p['stim_thresh'] = 0.0 # stimulus duration threshold for including trials\n",
    "p['align_to'] = ['clicks_on', 'clicks_on']\n",
    "p['align_name'] = ['clickson_masked', 'clickson_unmasked']\n",
    "p['pre_mask'] = [None, None]\n",
    "p['post_mask'] = ['clicks_off', None]\n",
    "p['start_time'] = [0, -100]\n",
    "p['end_time'] = [800, 1500] # in ms\n",
    "p['binsize'] = 5 # in ms\n",
    "p['filter_type'] = 'gaussian'\n",
    "p['filter_w'] = 25 # in ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dict()\n",
    "DV = dict()\n",
    "\n",
    "\n",
    "for reg in p['regions']:\n",
    "    X[reg], ntpts_per_trial = get_neural_activity(df_cell, df_trial, reg, p, 0)\n",
    "    \n",
    "    ntrials = len(ntpts_per_trial)\n",
    "    trial_idx = np.repeat(np.arange(ntrials), ntpts_per_trial)\n",
    "    DV[reg] = np.nan * np.zeros((ntrials, max(ntpts_per_trial)))\n",
    "    for tr in range(ntrials):\n",
    "        DV[reg][tr, :ntpts_per_trial[tr]] = summary[reg]['mdl_coefs'] @ X[reg][:, trial_idx == tr] + summary[reg]['mdl_intercept']\n",
    "    \n",
    "    \n",
    "num_tpts = np.shape(DV['A'])[1]\n",
    "DV_crosscoef = np.nan*np.zeros((ntrials, 2*num_tpts-1))\n",
    "for tr in range(ntrials):\n",
    "    nonnan = ~np.isnan(DV['A'][tr, :])\n",
    "    lags, values = xcorr(\n",
    "        DV['A'][tr, nonnan],\n",
    "        DV['B'][tr, nonnan],\n",
    "        detrend = False,\n",
    "        scale = 'unbiased'\n",
    "    )\n",
    "    DV_crosscoef[tr, lags + num_tpts -1] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = np.arange(-num_tpts+1, num_tpts) * p['binsize'] * 1e-3\n",
    "\n",
    "plt.plot(lags, np.nanmean(DV_crosscoef, axis = 0))\n",
    "lags = np.arange(-num_tpts+1, num_tpts) * p['binsize'] * 1e-3\n",
    "print(lags[np.argmax(np.nanmean(DV_crosscoef, axis = 0))])\n",
    "plt.axvline(0, c = 'k')\n",
    "plt.xlim([-0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tpts = np.shape(latents)[1]\n",
    "DV_crosscoef = np.nan*np.zeros((ntrials, 2*num_tpts-1))\n",
    "for tr in range(ntrials):\n",
    "    nonnan = ~np.isnan(DV['A'][tr, :])\n",
    "    lags, values = xcorr(\n",
    "        latents[tr, :, 0],\n",
    "        latents[tr, :, 3],\n",
    "        detrend = False,\n",
    "        scale = 'unbiased'\n",
    "    )\n",
    "    DV_crosscoef[tr, lags + num_tpts -1] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = np.arange(-num_tpts+1, num_tpts)  * 1e-3\n",
    "\n",
    "plt.plot(lags, np.nanmean(DV_crosscoef, axis = 0), zorder = 100)\n",
    "lags = np.arange(-num_tpts+1, num_tpts) * 1e-3\n",
    "print(lags[np.argmax(np.nanmean(DV_crosscoef, axis = 0))])\n",
    "plt.axvline(0, c = 'k')\n",
    "# plt.xlim([-0.05, 0.05])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop over\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_neurons = np.linspace(10, 100, 10).astype(int)\n",
    "N_trials = np.linspace(100, 300, 5).astype(int)\n",
    "ff_delays = np.linspace(1, 20, 5).astype(int)\n",
    "\n",
    "# for simulations\n",
    "params = dict()\n",
    "params['dt'] = 0.001\n",
    "params['history_bias'] = True\n",
    "params['bound'] = 8.\n",
    "params['leak'] = 0.99\n",
    "params['T'] = [0.2, 1.0]\n",
    "\n",
    "\n",
    "\n",
    "# for decoding\n",
    "p = dict()\n",
    "p['regions'] = ['A', 'B']\n",
    "p['cols'] = SPEC.COLS\n",
    "p['fr_thresh'] = 1.0 # firing rate threshold for including neurons\n",
    "p['stim_thresh'] = 0.0 # stimulus duration threshold for including trials\n",
    "p['align_to'] = ['clicks_on']\n",
    "p['align_name'] = ['clickson_masked']\n",
    "p['pre_mask'] = [None]\n",
    "p['post_mask'] = ['clicks_off']\n",
    "p['start_time'] = [0,]\n",
    "p['end_time'] = [1000] # in ms\n",
    "p['binsize'] = 50 # in ms\n",
    "p['filter_type'] = 'gaussian'\n",
    "p['filter_w'] = 75 # in ms\n",
    "p['Cs'] = np.logspace(-7,3,200)  # cross-validation parameter\n",
    "p['nfolds'] = 10  # number of folds for cross-validation\n",
    "p['n_repeats'] = 1 # number of repeats for cross-validation\n",
    "\n",
    "\n",
    "# for DV cross correlations\n",
    "prm = dict()\n",
    "prm['regions'] = ['A', 'B']\n",
    "prm['cols'] = SPEC.COLS\n",
    "prm['align_to'] = ['clicks_on']\n",
    "prm['align_name'] = ['clickson_masked']\n",
    "prm['pre_mask'] = [None]\n",
    "prm['post_mask'] = ['clicks_off']\n",
    "prm['start_time'] = [0]\n",
    "prm['end_time'] = [800] # in ms\n",
    "prm['binsize'] = 1 # in ms\n",
    "prm['filter_type'] = 'gaussian'\n",
    "prm['filter_w'] = 100 # in ms\n",
    "\n",
    "decoded_variable = 'pokedR'\n",
    "\n",
    "FIGSAVEPATH = SPEC.FIGUREDIR + \"figure2/DVsweeps/\"\n",
    "DATASAVEPATH = SPEC.RESULTDIR + \"figure2/DVsweeps/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DV_summary = dict()\n",
    "DV_summary['mdl_coefs'] = dict()\n",
    "DV_summary['corr_DVaxis_emissions'] = dict()\n",
    "DV_summary['DV'] = dict()\n",
    "\n",
    "\n",
    "# for nn in [N_neurons[:1]]:    \n",
    "#     for nt in N_trials[:1]:\n",
    "for ff in [10]:        \n",
    "    for nn in [10]:    \n",
    "        for nt in [100]:\n",
    "            \n",
    "            params['N_neurons_per_region'] = nn\n",
    "            params['N_batch'] = nt\n",
    "            params['ff_delay'] = ff\n",
    "            \n",
    "            for repeat in range(1):\n",
    "                \n",
    "                filename = \"DVsweep_num_trials_{}_num_neurons_{}_ffdelay_{}ms_repeat_{}\".format(nt, nn, ff, repeat)\n",
    "                \n",
    "                # simulate data\n",
    "                pc_task = PoissonClicks(**params)\n",
    "                ns = NeuralSimulator(**params)\n",
    "                click_data, task_data = pc_task.get_trial_batch()\n",
    "                latents, spikes, spike_rate_dt, task_data = ns.simulate_trials(click_data, task_data)\n",
    "                \n",
    "                # save some plots and process data\n",
    "                ns.plot_activity_portrait(task_data)\n",
    "                savethisfig(FIGSAVEPATH, filename + \"_activity\")\n",
    "                ns.plot_dynamics_params()\n",
    "                savethisfig(FIGSAVEPATH, filename + \"_parameters\")\n",
    "                df_trial, df_cell = make_dataframes(task_data, ns, spikes)\n",
    "                \n",
    "                # find DV axis\n",
    "                target = np.array(df_trial[decoded_variable], dtype = float)\n",
    "                summary = dict()\n",
    "                for r, reg in enumerate(p['regions']):\n",
    "                    X, ntpts_per_trial = get_neural_activity(df_cell, df_trial, reg, p, 0)\n",
    "                    summary[reg] = run_logistic_decoding(X, target, ntpts_per_trial, p)\n",
    "                    DV_summary['mdl_coefs'][reg] = summary[reg]['mdl_coefs']\n",
    "                    DV_summary['corr_DVaxis_emissions'][reg] = np.corrcoef(\n",
    "                        summary[reg]['mdl_coefs'], \n",
    "                        ns.C[r*nn:(r+1)*nn, r*ns.N_latents_per_region])\n",
    "                                    \n",
    "                # compute DVs\n",
    "                X = dict()\n",
    "                for reg in prm['regions']:\n",
    "                    X[reg], ntpts = get_neural_activity(df_cell, df_trial, reg, prm, 0)\n",
    "                    trial_idx = np.repeat(np.arange(nt), ntpts)\n",
    "                    DV_summary['DV'][reg] = np.nan * np.zeros((nt, max(ntpts)))\n",
    "                    for tr in range(nt):\n",
    "                        this_DV = summary[reg]['mdl_coefs'] @ X[reg][:, trial_idx == tr] + summary[reg]['mdl_intercept']\n",
    "                        DV_summary['DV'][reg][tr, :ntpts[tr]] = this_DV\n",
    "                \n",
    "                # cross correlate DVs\n",
    "                DV_summary['DV_cc'] = crosscorrelate_DVs(DV_summary['DV'], prm, shuffle = False)\n",
    "                DV_summary['DV_cc_shuff'] = crosscorrelate_DVs(DV_summary['DV'], prm, shuffle = True)\n",
    "                \n",
    "                # plot and save data\n",
    "                plot_cross_corr_metrics(DV_summary, ns, summary, df_trial, latents)\n",
    "                savethisfig(FIGSAVEPATH, filename + \"_DVcc\")\n",
    "                \n",
    "                \n",
    "            # Create a dictionary to hold the named dictionaries\n",
    "            all_dicts = {\n",
    "                'decoder_p': p, \n",
    "                'DV_p': prm, \n",
    "                'ns_p': ns.get_params(), \n",
    "                'dec_summary': summary, \n",
    "                'DV_summary': DV_summary}\n",
    "\n",
    "            # Save the dictionary containing all named dictionaries to a file\n",
    "            with open(DATASAVEPATH + filename + '.pkl', 'wb') as file:\n",
    "                pickle.dump(all_dicts, file)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    N_trials = np.linspace(100, 300, 5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_repeats = 5\n",
    "\n",
    "for repeat in range(N_repeats):\n",
    "    print(repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns.plot_activity_portrait(task_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plotting summary of simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re\n",
    "sys.path.insert(1, '../../../figure_code/')\n",
    "from my_imports import *\n",
    "\n",
    "# sns.set_theme(context='paper', \n",
    "#               style='ticks',  \n",
    "#               font='Helvetica', \n",
    "#               font_scale=1.3,  \n",
    "#               rc={\"axes.titlesize\": 13})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMDIR = SPEC.RESULTDIR + \"figure2/DVsweeps/\"\n",
    "all_files = sorted(os.listdir(SIMDIR))\n",
    "\n",
    "columns = ['num_neurons', 'num_trials', 'ff_delay', 'peak_time']\n",
    "df = pd.DataFrame(columns = columns)\n",
    "\n",
    "for f in all_files:\n",
    "    p = np.load(SIMDIR + f, allow_pickle=True).item()\n",
    "    data = {'num_neurons': p['params']['N_neurons_per_region'],\n",
    "            'num_trials': p['params']['N_batch'],\n",
    "            'ff_delay': p['params']['ff_delay'],\n",
    "            'peak_time': p['DV_cc']['peak']}\n",
    "    \n",
    "    df = df.append(data, ignore_index = True)\n",
    "    \n",
    "df['num_neurons'] = df['num_neurons'].astype(int)\n",
    "df['num_trials'] = df['num_trials'].astype(int)\n",
    "df['ff_delay'] = df['ff_delay'].astype(int)\n",
    "df['peak_time'] = df['peak_time']*1000\n",
    "\n",
    "grouped = df.groupby(['num_neurons', 'num_trials', 'ff_delay']).agg({'peak_time': ['mean', 'std']})\n",
    "grouped = grouped.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ff_delays = grouped['ff_delay'].unique()\n",
    "num_delays = len(unique_ff_delays)\n",
    "fig, axs = plt.subplots(num_delays, figsize = (2, 1.5*num_delays), sharex = True, sharey = True)\n",
    "\n",
    "for i, ff in enumerate(unique_ff_delays):\n",
    "    filtered_df = grouped[grouped['ff_delay'] == ff]\n",
    "    imshow_mean = filtered_df.pivot(index='num_trials', columns='num_neurons', values=('peak_time','mean'))\n",
    "    imshow_std = filtered_df.pivot(index='num_trials', columns='num_neurons', values=('peak_time','std'))\n",
    "\n",
    "    im = axs[i].imshow(imshow_mean, cmap='RdBu', aspect='auto', origin='lower', vmin = -50, vmax = 50)\n",
    "    axs[i].set_title('True lag = {}ms'.format(-ff), fontsize = 8)\n",
    "    axs[i].set_xticks(range(len(imshow_mean.columns)), imshow_mean.columns, fontsize = 8)\n",
    "    axs[i].set_yticks(range(len(imshow_mean.index)), imshow_mean.index, fontsize = 8)\n",
    "\n",
    "# Set common x and y labels\n",
    "fig.text(0.5, 0.06, 'Number of neurons \\n(per region)', ha='center', fontsize = 8)\n",
    "fig.text(-0.1, 0.5, 'Number of trials', va='center', rotation='vertical', fontsize = 8)\n",
    "\n",
    "# add the colorbar at the top of the plot\n",
    "cbar_ax = fig.add_axes([0.15, 0.95, 0.7, 0.015])  # [left, bottom, width, height]\n",
    "cbar = plt.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "cbar_ax.set_title('Inferred lag \\n(ms; peak of DV crosscorrelation)', fontsize = 8)\n",
    "cbar.ax.tick_params(labelsize=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imshow_data, cmap='RdBu', aspect='auto', origin='lower')\n",
    "plt.xticks(range(len(imshow_data.columns)), imshow_data.columns)\n",
    "plt.yticks(range(len(imshow_data.index)), imshow_data.index)\n",
    "plt.colorbar(label='Peak Time')\n",
    "plt.xlabel('num_neurons')\n",
    "plt.ylabel('num_trials')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ff in unique_ff_delays:\n",
    "    \n",
    "    filtered_df = grouped[grouped['ff_delay'] == ff]\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(filtered_df[('peak_time','mean')].values.reshape(-1,2), cmap = 'viridis', aspect = 'auto')\n",
    "    plt.colorbar(label = \"inferred Delay\")\n",
    "    plt.xlabel('Number of neurons \\n(per region)')\n",
    "    plt.ylabel('Number of trials')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean of D for each combination of A, B, and C\n",
    "grouped_mean = df.groupby(['A', 'B', 'C'])['D'].mean().reset_index()\n",
    "\n",
    "# Iterate through unique values of C\n",
    "unique_C_values = grouped_mean['C'].unique()\n",
    "\n",
    "for c_value in unique_C_values:\n",
    "    # Filter DataFrame for the current value of C\n",
    "    filtered_df = grouped_mean[grouped_mean['C'] == c_value]\n",
    "    \n",
    "    # Create the imshow plot\n",
    "    plt.figure()\n",
    "    plt.imshow(filtered_df['D'].values.reshape(-1, 2), cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(label='Mean of D')\n",
    "    plt.xlabel('A')\n",
    "    plt.ylabel('B')\n",
    "    plt.title(f'imshow Plot for C = {c_value}')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssmrun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
