{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659d0cc-6f2f-47f0-86b9-cd05993b85ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# glm related imports\n",
    "from common_imports import *\n",
    "from my_imports import *\n",
    "from train_model import *\n",
    "from glmfits_utils import *\n",
    "from simulations import *\n",
    "from basis_kernels import *\n",
    "\n",
    "# data loading related imports\n",
    "from helpers.phys_helpers import get_sortCells_for_rat, datetime, split_trials, flatten_list, savethisfig\n",
    "from helpers.physdata_preprocessing import load_phys_data_from_Cell\n",
    "from helpers.rasters_and_psths import get_neural_activity\n",
    "\n",
    "# logistic decoding related imports\n",
    "sys.path.insert(1, '../../../figure_code/')\n",
    "from figure2.fig2_helpers.logisticdecoding import run_logistic_decoding, equalize_trials\n",
    "from figure2.fig2_helpers.evidencedecoding import get_evidence, run_evidence_decoding\n",
    "\n",
    "from sklearn.linear_model import LassoCV, LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "\n",
    "# figure plotting settings\n",
    "# plt.style.use('seaborn-deep')\n",
    "plt.rcParams['figure.dpi']= 150\n",
    "plt.rcParams['figure.figsize'] = [4, 3]\n",
    "plt.rcParams[\"font.family\"] = \"Helvetica\"\n",
    "plt.rcParams['axes.titlesize'] = 10\n",
    "plt.rcParams['xtick.major.size'] = 2 \n",
    "plt.rcParams['ytick.major.size'] = 2 \n",
    "# plt.rcParams['axes.prop_cycle'] = plt.cycler(\"color\", plt.cm.RdGy(np.linspace(0,1,8)))\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(\"color\", plt.cm.Spectral(np.linspace(0,1,8)))\n",
    "plt.rcParams['figure.max_open_warning'] = 0\n",
    "\n",
    "\n",
    "sns.set_context('paper')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b29529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CV_logll(\n",
    "    GLM_DIR,\n",
    "    file,\n",
    "    region_X,\n",
    "    region_Y):\n",
    "    \n",
    "    data = np.load(GLM_DIR + file, allow_pickle=True).item()\n",
    "    this_fit = data['model_dict'][region_X + '_' + region_Y]\n",
    "    val_loss_per_spike = []\n",
    "    for repeat in range(data['p']['num_repeats']):\n",
    "        val_loss = []\n",
    "        for fold in range(1, data['p']['num_folds']+1):\n",
    "            val_loss.append(-1*this_fit[repeat][fold]['validation_loss'])\n",
    "        val_loss_per_spike.append(np.mean(val_loss))\n",
    "        \n",
    "    meta = dict()\n",
    "    meta['mean_val_loss_per_spike'] = np.mean(val_loss_per_spike)\n",
    "    meta['sem_val_loss_per_spike'] = np.std(val_loss_per_spike)/np.sqrt(len(val_loss_per_spike))\n",
    "    meta['filename'] = data['filename']\n",
    "    meta['fit_filename'] = file\n",
    "    return meta\n",
    "\n",
    "\n",
    "# GET SUMMARY\n",
    "RRGLM_DIR = SPEC.RESULTDIR + 'neural_rr_GLM/'\n",
    "ranks = range(1,6)\n",
    "summary = pd.DataFrame()\n",
    "for (region_X, region_Y) in zip(['FOF', 'ADS'], ['ADS', 'FOF']):\n",
    "    for rank in ranks:\n",
    "        rrglm_files = sorted([fn for fn in os.listdir(RRGLM_DIR) if '.npy' in fn])\n",
    "        rrglm_files = [fn for fn in rrglm_files if f'rank{rank}_25-05-2024' in fn or f'rank{rank}_26-05-2024' in fn or f'rank{rank}_27-05-2024' in fn]\n",
    "        \n",
    "        for filename in rrglm_files:\n",
    "            meta = get_CV_logll(RRGLM_DIR, filename, region_X,region_Y)\n",
    "            meta['rank'] = rank\n",
    "            meta['fit_type'] = [region_X + '_' + region_Y] \n",
    "            summary = pd.concat([summary, pd.DataFrame(meta)], ignore_index = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be12163d",
   "metadata": {},
   "source": [
    "### find optimal ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca33994",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_rank_dict = dict()\n",
    "filenames = np.unique(summary['filename'])\n",
    "\n",
    "for f, fit_type in enumerate(np.unique(summary[\"fit_type\"])):\n",
    "    optimal_rank_dict[fit_type] = dict()\n",
    "    for i, file in enumerate(filenames):  \n",
    "        optimal_rank_dict[fit_type][file] = dict()\n",
    "        \n",
    "        this_df = summary.query(\"filename == '{}' & fit_type == '{}'\".format(file, fit_type) )\n",
    "        ranks = np.unique(this_df[\"rank\"])\n",
    "        y = np.array([this_df[this_df[\"rank\"] == rank]['mean_val_loss_per_spike'].values[0] for rank in ranks])\n",
    "        \n",
    "        optimal_rank = min(4, ranks[np.argmax(y)])\n",
    "        fit_filename = this_df.query(\"rank == {}\".format(optimal_rank))['fit_filename'].values[0]\n",
    "        optimal_rank_dict[fit_type][file]['rank'] = optimal_rank\n",
    "        optimal_rank_dict[fit_type][file]['fit_filename'] = fit_filename\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78249a26",
   "metadata": {},
   "source": [
    "### plot logll as a function of rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999073c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = np.unique(summary['filename'])[:-1]\n",
    "fig, axs = plt.subplots(2,2, figsize = (4,3), sharex = True)\n",
    "\n",
    "for f, fit_type in enumerate(np.unique(summary[\"fit_type\"])):\n",
    "    \n",
    "    y_overall = []\n",
    "    for i, file in enumerate(filenames):    \n",
    "        this_df = summary.query(\"filename == '{}' & fit_type == '{}'\".format(file, fit_type) )\n",
    "        ranks = np.unique(this_df[\"rank\"])\n",
    "        \n",
    "        y = np.array([this_df[this_df[\"rank\"] == rank]['mean_val_loss_per_spike'].values[0] for rank in ranks])\n",
    "        yerr = [this_df[this_df[\"rank\"] == rank]['sem_val_loss_per_spike'].values[0] for rank in ranks]\n",
    "        min_y, max_y = np.min(y), np.max(y)\n",
    "        y = (y - min_y)/(max_y - min_y)\n",
    "        y_overall.append(y)\n",
    "        axs[0,f].errorbar(ranks, y, yerr=yerr, fmt='-o', alpha = 0.5, ms = 2)\n",
    "        axs[0,f].set_xticks(range(1,6))\n",
    "        \n",
    "        if fit_type == 'ADS_FOF':\n",
    "            axs[0,f].set_title(\"ADS $\\\\rightarrow$ FOF\")\n",
    "        else:\n",
    "            axs[0,f].set_title(\"FOF $\\\\rightarrow$ ADS\")\n",
    "        \n",
    "    y_overall_mean = np.mean(y_overall, 0)\n",
    "    y_overall_sem = np.std(y_overall, 0)/np.sqrt(len(filenames))\n",
    "    axs[0,f].errorbar(ranks, y_overall_mean, yerr = y_overall_sem, fmt = '-o', c = 'k', ms = 3, zorder = 100)\n",
    "    \n",
    "    # plot picked ranks\n",
    "    picked_ranks = [optimal_rank_dict[fit_type][file]['rank'] for file in filenames]\n",
    "    axs[1,f].hist(picked_ranks, color = 'grey', edgecolor = 'k')\n",
    "    axs[1,f].set_ylim(0,10)\n",
    "\n",
    "fig.text(0.6, 0.0, 'Rank of comm. subspace (CS)', ha='center')\n",
    "axs[0,0].set_ylabel('Norm. C.V. logll \\nper spike $\\pm$ SEM')\n",
    "axs[1,0].set_ylabel('Number of sessions \\n with rank')\n",
    "\n",
    "fig.align_ylabels(axs[:, 0])\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "savethisfig(SPEC.FIGUREDIR + \"figure2/\", 'figure2_CSrank')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3924606b",
   "metadata": {},
   "source": [
    "### Plot r2 distributions across datasets for optimal ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a3120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bestfit_params(GLM_DIR, file, region_X, region_Y):\n",
    "\n",
    "    data = np.load(GLM_DIR + file, allow_pickle=True).item()\n",
    "    \n",
    "    prm_meta = dict()\n",
    "    prm_meta['filename'] = data['filename']\n",
    "    prm_meta['covar_dict'] = data['covar_dict'] \n",
    "    prm_meta['rank_dict'] = data['rank_dict']\n",
    "    prm_meta['p'] = data['p']\n",
    "    \n",
    "    data = data['model_dict'][region_X + '_' + region_Y]\n",
    "    prm_meta['best_reg_param'] = data['best_reg_param']\n",
    "    prm_meta['best_avg_val_loss'] = data['best_avg_val_loss']\n",
    "    prm_meta['loss'] = data['loss']\n",
    "\n",
    "    return data['state_dict'], prm_meta\n",
    "\n",
    "\n",
    "\n",
    "def get_data_for_glm(filename, p, region_X, region_Y):\n",
    "    \n",
    "    _, this_datadir = get_sortCells_for_rat(filename[:4], SPEC.DATADIR)\n",
    "    df_trial, df_cell, _ = load_phys_data_from_Cell(this_datadir + os.sep + filename)\n",
    "    df_cell = df_cell[df_cell.stim_fr > p['fr_thresh']].reset_index()\n",
    "\n",
    "    X = get_X(df_trial, p)\n",
    "    Y = get_Y(df_cell, df_trial, p, region = region_Y)\n",
    "    \n",
    "    p_input = deepcopy(p)\n",
    "    p_input['window'] = [x - p_input['binsize'] for x in p_input['window']]\n",
    "    Y_input = get_Y(df_cell, df_trial, p_input, region = region_X)\n",
    "    \n",
    "    return X, Y_input, Y, df_trial, df_cell\n",
    "\n",
    "\n",
    "\n",
    "def load_glm_with_params(this_glm, state_dict):\n",
    "    \n",
    "    model_var_list = list(state_dict)\n",
    "    model_var_list = [item for item in model_var_list if (item != 'bias' and item != 'rr_basis_w')]\n",
    "    model_variables = [item.replace('.weight', '') for item in model_var_list]\n",
    "    for val, val_id in zip(model_variables, model_var_list):\n",
    "        getattr(this_glm, val).weight.data = to_t(state_dict[val_id])\n",
    "    this_glm.bias = nn.Parameter(to_t(state_dict['bias']))\n",
    "    this_glm.rr_basis_w = nn.Parameter(to_t(state_dict['rr_basis_w']))\n",
    "\n",
    "    return this_glm\n",
    "\n",
    "\n",
    "\n",
    "def load_glm_model(GLM_DIR, file, data, region_X, region_Y, num_X, num_Y):\n",
    "    \n",
    "    state_dict, data = get_bestfit_params(GLM_DIR, file, region_X, region_Y)\n",
    "    data['rank_dict']['num_inputs'] = num_X\n",
    "    this_glm = glm_rr(\n",
    "            covar_dict = data['covar_dict'],\n",
    "            num_Y = num_Y,\n",
    "            rank_dict = data['rank_dict'],\n",
    "            dt = data['p']['dt']).to(device)\n",
    "    this_glm = load_glm_with_params(this_glm, state_dict)\n",
    "    return this_glm \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_CV_predictions(\n",
    "    GLM_DIR, \n",
    "    file, \n",
    "    region_X,\n",
    "    region_Y,\n",
    "    psth_filter_w = 75, \n",
    "    psth_filter_type = 'gaussian'):\n",
    "    \n",
    "    data = np.load(GLM_DIR + file, allow_pickle=True).item()\n",
    "    this_fit = data['model_dict'][region_X + '_' + region_Y][0]\n",
    "    \n",
    "    X, Y_input, Y, df_trial, df_cell = get_data_for_glm(\n",
    "        data['filename'], \n",
    "        data['p'], \n",
    "        region_X,\n",
    "        region_Y)\n",
    "    df_cell = df_cell.loc[df_cell['region'] == region_Y].reset_index(drop = True)\n",
    "    dataset = glmRRDataset(X,Y_input,Y)\n",
    "    \n",
    "    this_glm = load_glm_model(GLM_DIR, file, data, region_X, region_Y, Y_input.shape[2], Y.shape[2])\n",
    "    \n",
    "    yhat = np.nan * np.zeros(np.shape(Y))\n",
    "    for fold in range(1,data['p']['num_folds']+1):\n",
    "        this_glm = load_glm_with_params(this_glm, this_fit[fold]['state_dict'])\n",
    "        this_dataset = dataset[this_fit[fold]['val_index']]\n",
    "        preds = this_glm(\n",
    "            this_dataset['stimulus'], \n",
    "            this_dataset['input_spikes'], \n",
    "            this_dataset['spikes'])\n",
    "        yhat[this_fit[fold]['val_index'], :, :] = from_t(preds)\n",
    "        \n",
    "    p_psth = copy.deepcopy(data['p'])\n",
    "    p_psth['filter_w'] = psth_filter_w\n",
    "    p_psth['filter_type'] = psth_filter_type\n",
    "    y =  get_Y(df_cell, df_trial, p_psth, region = region_Y)\n",
    "    \n",
    "    meta = dict()\n",
    "    meta['cell_ID'] = df_cell['cell_ID']\n",
    "    meta['firing_rate'] = np.nanmean(y, (0,1))/data['p']['dt']\n",
    "    meta['region'] = df_cell['region']\n",
    "    meta['auc'] = df_cell['auc']\n",
    "    meta['side_pref'] = df_cell['side_pref']\n",
    "    meta['side_pref_p'] = df_cell['side_pref_p']\n",
    "    meta['stim_fr'] = df_cell['stim_fr']\n",
    "    meta['filename'] = data['filename']\n",
    "    meta['ratname'] = data['filename'][:4]\n",
    "    meta['fit_filename'] = file\n",
    "\n",
    "    return yhat, y, df_trial, meta\n",
    "\n",
    "\n",
    "    \n",
    "def compute_R2(yhat, y, df_trial, split_by = 'pokedR'):\n",
    "    \n",
    "    if split_by is not None:\n",
    "        splits = split_trials(df_trial, split_by)\n",
    "    else:\n",
    "        splits = None\n",
    "        \n",
    "    this_r2 = []\n",
    "    for cell_num in range(y.shape[2]):\n",
    "        this_y = np.squeeze(y[:,:,cell_num])\n",
    "        this_yhat = np.squeeze(yhat[:,:,cell_num])\n",
    "        this_r2.append(compute_R2_travg(this_yhat, this_y, splits))\n",
    "        \n",
    "    return np.array(this_r2)\n",
    "\n",
    "\n",
    "def compute_R2_travg(yhat, y, splits):\n",
    "    \n",
    "    if splits is not None:\n",
    "        r2_num, r2_den = 0., 0.\n",
    "        conds = list(splits)\n",
    "        mean_PSTH = np.nanmean(np.vstack([y[splits[c],:] for c in conds]), axis = 0)\n",
    "        for c in conds:\n",
    "            cond_mean = np.nanmean(y[splits[c],:], axis = 0)\n",
    "            pred_cond_mean = np.nanmean(yhat[splits[c],:], axis = 0)\n",
    "            r2_num += np.nansum((cond_mean - pred_cond_mean)**2)\n",
    "            r2_den += np.nansum((mean_PSTH - cond_mean)**2)\n",
    "    else:\n",
    "        meanfr = np.nanmean(y)\n",
    "        pred_meanfr = np.nanmean(yhat)\n",
    "        meanPSTH = np.nanmean(y, axis = 0)\n",
    "        pred_meanPSTH = np.nanmean(yhat, axis = 0)\n",
    "        r2_num = np.nansum((meanPSTH - pred_meanPSTH)**2)\n",
    "        r2_den = (meanfr - pred_meanfr)**2\n",
    "\n",
    "    return (1 - (r2_num / r2_den)) \n",
    "\n",
    "\n",
    "def plot_R2_vs_firing_rate(data):\n",
    "        \n",
    "    col_pal = {tuple(SPEC.COLS[key]) for key in list(SPEC.COLS)}\n",
    "    data.r2 = data.r2.clip(-0.2, 1)\n",
    "    g = sns.JointGrid(ratio = 2, height = 6)\n",
    "    sns.scatterplot(data = data, x = \"firing_rate\", y = \"r2\", hue = \"region\", \n",
    "                    palette = col_pal, s=20, alpha = .5, ax=g.ax_joint)\n",
    "    g.ax_joint.set_ylim([-0.1,1.05])\n",
    "    g.ax_joint.axhline(0., c= [0.6, 0.6, 0.6], ls = '--', zorder = 0)\n",
    "    sns.histplot(\n",
    "        data = data, y = \"r2\", hue = \"region\", \n",
    "        stat = \"probability\", common_norm = False, binwidth = 0.05, \n",
    "        palette = col_pal, binrange = [-0.2, 1],\n",
    "        ax=g.ax_marg_y, legend = False)\n",
    "    sns.histplot(\n",
    "        data = data, x = \"firing_rate\", hue = \"region\", \n",
    "        stat = \"probability\", common_norm = False, bins = 30,\n",
    "        palette = col_pal, ax=g.ax_marg_x, legend = False);\n",
    "    g.ax_joint.set_xlabel(\"Firing Rate (spk/s)\", fontsize = 14)\n",
    "    g.ax_joint.set_ylabel(\"Cross validated R2\", fontsize = 14)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d01cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RRGLM_DIR = SPEC.RESULTDIR + 'neural_rr_GLM/'\n",
    "\n",
    "psth_filter_w = 75\n",
    "psth_filter_type = 'gaussian'\n",
    "split_by = 'pokedR'\n",
    "summary = pd.DataFrame()\n",
    "\n",
    "for (region_X, region_Y) in zip(['FOF', 'ADS'], ['ADS', 'FOF']):\n",
    "    print(region_X, region_Y)\n",
    "    fit_type = \"{}_{}\".format(region_X, region_Y)\n",
    "    for data_file in list(optimal_rank_dict[fit_type]):\n",
    "        print(optimal_rank_dict[fit_type][data_file][\"fit_filename\"])\n",
    "        yhat, y, df_trial, meta = get_CV_predictions(\n",
    "            RRGLM_DIR, \n",
    "            optimal_rank_dict[fit_type][data_file]['fit_filename'], \n",
    "            region_X,\n",
    "            region_Y,\n",
    "            psth_filter_w = psth_filter_w, \n",
    "            psth_filter_type = psth_filter_type)\n",
    "        \n",
    "        meta['r2'] = compute_R2(yhat, y, df_trial, split_by = 'pokedR')\n",
    "        meta['rank'] = np.repeat(rank, y.shape[2])\n",
    "        meta['fit_type'] = [region_X + '_' + region_Y] * y.shape[2]\n",
    "        summary = pd.concat([summary, pd.DataFrame(meta)], ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c9380",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_R2_vs_firing_rate(summary)\n",
    "savethisfig(SPEC.FIGUREDIR + \"figure2/\", 'figure2_rrglm_R2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd600a34",
   "metadata": {},
   "source": [
    "### Print some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458fec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraction of neurons with R2< 0\n",
    "\n",
    "for region in np.unique(summary.region):\n",
    "    this_df = summary.query(\"region == '{}'\".format(region))\n",
    "    \n",
    "    frac =  sum((this_df.r2 < 0) == True)/len(this_df.r2)\n",
    "    print(\"\\n\\nFraction less than 0 R2: {}, {}\".format(region, frac))\n",
    "    \n",
    "    this_df = summary.query(\"region == '{}' and r2 >= 0\".format(region))\n",
    "    meanR2 =  np.mean(this_df.r2)\n",
    "    semR2 = np.std(this_df.r2)/np.sqrt(len(this_df.r2))\n",
    "    print(\"\\nMeanR2 greater than 0 R2: {}, {} Â± {}\".format(region, meanR2, semR2))\n",
    "    \n",
    "    medianR2 =  np.median(this_df.r2)\n",
    "    print(\"MedianR2 greater than 0 R2: {}, {}\".format(region, medianR2))\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "492e7d3a",
   "metadata": {},
   "source": [
    "### run linear evidence decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd7d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evidence(ntpts_per_trial, df_trial, p, ev_type=\"delta\"):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        ntpts_per_trial (_type_): _description_\n",
    "        df_trial (_type_): _description_\n",
    "        p (_type_): _description_\n",
    "        ev_type (str, optional): _description_. Defaults to \"delta\".\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"aligning to clicks on ignoring definition in p\")\n",
    "    max_tpts = np.max(ntpts_per_trial)\n",
    "    evidence = np.nan * np.zeros((len(df_trial), max_tpts))\n",
    "    for tr, ntpts in zip(range(len(df_trial)), ntpts_per_trial):\n",
    "        edges = np.arange(p['start_time'][0], (ntpts+1)*p['binsize'], p['binsize'])*0.001\n",
    "        counts_L, _ = np.histogram(df_trial['leftBups'][tr] - df_trial['clicks_on'][tr], edges)\n",
    "        counts_R, _ = np.histogram(df_trial['rightBups'][tr] - df_trial['clicks_on'][tr], edges)\n",
    "        if ev_type == \"delta\":\n",
    "            cumsum_data = np.cumsum(counts_R - counts_L)\n",
    "        elif ev_type == \"right\":\n",
    "            cumsum_data = np.cumsum(counts_R)\n",
    "        elif ev_type == \"left\":\n",
    "            cumsum_data = np.cumsum(counts_L)\n",
    "\n",
    "        # Pad with zeros at the beginning if needed\n",
    "        if len(cumsum_data) < ntpts:\n",
    "            padding_length = ntpts - len(cumsum_data)\n",
    "            padded_data = np.concatenate([np.zeros(padding_length), cumsum_data])\n",
    "            evidence[tr, :ntpts] = padded_data\n",
    "        elif len(cumsum_data) > ntpts:\n",
    "            # Truncate if longer than ntpts\n",
    "            evidence[tr, :ntpts] = cumsum_data[:ntpts]\n",
    "        else:\n",
    "            # Exact length match\n",
    "            evidence[tr, :ntpts] = cumsum_data\n",
    "    return evidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f78163",
   "metadata": {},
   "outputs": [],
   "source": [
    "RRGLM_DIR = SPEC.RESULTDIR + 'neural_rr_GLM/'\n",
    "\n",
    "p = dict()\n",
    "p['ratnames'] = SPEC.RATS\n",
    "p['regions'] = SPEC.REGIONS\n",
    "p['cols'] = SPEC.COLS\n",
    "p['fr_thresh'] = 1.0  # firing rate threshold for including neurons\n",
    "p['stim_thresh'] = 0.0  # stimulus duration threshold for including trials\n",
    "p['align_to'] = ['clicks_on_delayed']\n",
    "p['align_name'] = ['clickson_masked']\n",
    "p['pre_mask'] = [None]\n",
    "p['post_mask'] = ['clicks_off_delayed']\n",
    "p['start_time'] = [-100]\n",
    "p['end_time'] = [1100] # in ms   \n",
    "p['trial_type'] = ['all']\n",
    "p['Tshift'] = 100 # delay for stimulus (in ms)\n",
    "p['binsize'] = 50  # in ms\n",
    "p['filter_type'] = 'gaussian'\n",
    "p['filter_w'] = 75  # in ms\n",
    "p['Cs'] = np.logspace(-8,8,400)  # cross-validation parameter\n",
    "p['nfolds'] = 10    # number of folds for cross-validation\n",
    "p['n_repeats'] = 10 # number of repeats for cross-validation\n",
    "\n",
    "\n",
    "for fit_type in list(optimal_rank_dict):\n",
    "    \n",
    "    region_x, region_y = fit_type.split('_')\n",
    "\n",
    "    for data_file in list(optimal_rank_dict[fit_type]):\n",
    "\n",
    "        fit_file = optimal_rank_dict[fit_type][data_file]['fit_filename']\n",
    "        data = np.load(RRGLM_DIR + fit_file, allow_pickle=True).item()\n",
    "        p_rrglm = data['p']\n",
    "        p_rrglm['n_repeats'] = p['n_repeats']\n",
    "        p_rrglm['nfolds'] = p['nfolds']\n",
    "        p_rrglm['Cs'] = [0]\n",
    "        p_rrglm['slide_t'] = 50\n",
    "        p['binsize'] = p_rrglm['binsize']\n",
    "\n",
    "\n",
    "        rat = data['filename'][:4]\n",
    "        print(\"\\n\\n\\n\\n===== RAT: {} =====\".format(rat))\n",
    "        files, this_datadir = get_sortCells_for_rat(rat, SPEC.DATADIR)\n",
    "        df_trial, df_cell, _ = load_phys_data_from_Cell(this_datadir + os.sep + data['filename'])\n",
    "        df_trial = df_trial[df_trial['stim_dur_s_actual'] >= p['stim_thresh']].reset_index(drop = True)\n",
    "        df_trial = equalize_trials(df_trial, 'pokedR')\n",
    "\n",
    "        # not equalizing neurons across regions\n",
    "        df_cell = df_cell[df_cell['stim_fr'] >= p['fr_thresh']].reset_index(drop = True)\n",
    "\n",
    "        df_trial['clicks_on_delayed'] = df_trial['clicks_on'] + 1e-3*p['Tshift']\n",
    "        df_trial['clicks_off_delayed'] = df_trial['clicks_off'] + 1e-3*p['Tshift']\n",
    "\n",
    "        SAVEDIR = RRGLM_DIR + 'decoding/'\n",
    "        fname = SAVEDIR + 'params' + datetime()[5:] + \".npy\"\n",
    "        np.save(fname, p)\n",
    "\n",
    "        summary = dict()\n",
    "        fname = SAVEDIR + 'decoding_' + fit_type + '_' + data_file[:-4] + datetime()[5:] + '.npy'\n",
    "\n",
    "\n",
    "        ### DECODE FROM WHOLE REGION\n",
    "        summary[region_y] = dict()\n",
    "        print(\"\\n\\nDecoding from region: {} =====\".format(region_y))\n",
    "        X, ntpts_per_trial = get_neural_activity(df_cell, df_trial, region_y, p, 0)\n",
    "        evidence = get_evidence(ntpts_per_trial, df_trial, p)\n",
    "        \n",
    "        summary[region_y] = run_evidence_decoding(X, evidence, ntpts_per_trial, p)\n",
    "            \n",
    "        ### DECODE FROM GLM\n",
    "        model_var_list = list(data['model_dict'][fit_type]['state_dict'])\n",
    "        model_var_list = [item for item in model_var_list if (item != 'bias' and item != 'rr_basis_w')]\n",
    "        model_variables = [item.replace('.weight', '') for item in model_var_list]\n",
    "\n",
    "        X = get_X(df_trial, p_rrglm)\n",
    "\n",
    "    \n",
    "        print(\"\\n\\nDecoding from CS: {} =====\".format(fit_type))\n",
    "        summary[fit_type] = dict()\n",
    "        Y = get_Y(df_cell, df_trial, p_rrglm, region = region_y)\n",
    "        num_Y = Y.shape[2]\n",
    "\n",
    "        p_input = deepcopy(p_rrglm)\n",
    "        p_input['window'] = [x - p_input['binsize'] for x in p_input['window']]\n",
    "        Y_input = get_Y(df_cell, df_trial, p_input, region = region_x)\n",
    "        data['rank_dict']['num_inputs'] = Y_input.shape[2]\n",
    "\n",
    "        dataset = glmRRDataset(X, Y_input, Y)\n",
    "        \n",
    "        this_glm = glm_rr(\n",
    "            covar_dict = data['covar_dict'],\n",
    "            num_Y = num_Y,\n",
    "            rank_dict = data['rank_dict'],\n",
    "            dt = p_rrglm['dt']).to(device)\n",
    "\n",
    "        this_state_dict = data['model_dict'][fit_type]['state_dict']\n",
    "\n",
    "        for val, val_id in zip(model_variables, model_var_list):\n",
    "            getattr(this_glm, val).weight.data = to_t(this_state_dict[val_id])\n",
    "        this_glm.bias = nn.Parameter(to_t(this_state_dict['bias']))\n",
    "        this_glm.rr_basis_w = nn.Parameter(to_t(this_state_dict['rr_basis_w']))\n",
    "        \n",
    "        # from the activity after being filtered\n",
    "        _, output = this_glm.preds_rr(\n",
    "            dataset.stimulus,\n",
    "            dataset.input_spikes,\n",
    "            dataset.spikes)\n",
    "        output = from_t(output)\n",
    "        \n",
    "        # Now align the activity\n",
    "        aligned_output = []\n",
    "        for tnum in range(len(df_trial)):\n",
    "            start_index = 1000*(df_trial.loc[tnum, p['align_to'][0]] + (p['start_time'][0]/1000) - df_trial.loc[tnum, p_rrglm['align_to']])/p_rrglm['binsize']\n",
    "            start_index = int(np.floor(start_index))\n",
    "            aligned_output.append(output[tnum, range(start_index, start_index + ntpts_per_trial[tnum])])\n",
    "        aligned_output = np.array(flatten_list(aligned_output)).T\n",
    "        \n",
    "        # and then decode\n",
    "        summary[fit_type] = run_evidence_decoding(aligned_output, evidence, ntpts_per_trial, p_rrglm)\n",
    "        summary[fit_type]['rr_vec'] = this_state_dict['rr_U_w.weight']\n",
    "\n",
    "        # np.save(fname, summary)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a04457a",
   "metadata": {},
   "source": [
    "## load the fits up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b479ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_rank_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a851b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EV_DECODING_DIR = SPEC.RESULTDIR + 'neural_rr_GLM/decoding/'\n",
    "decoding_files = sorted([fn for fn in os.listdir(EV_DECODING_DIR) if '.npy' in fn and 'decoding' in fn and '28-05-2024' in fn])\n",
    "\n",
    "decoding_results = dict()\n",
    "\n",
    "for fit_type in list(optimal_rank_dict):\n",
    "    region_x, region_y = fit_type.split('_')\n",
    "    decoding_results[region_y] = []\n",
    "    decoding_results[fit_type] = []\n",
    "    this_decoding_files = sorted([fn for fn in decoding_files if fit_type in fn])\n",
    "    \n",
    "    for this_file in this_decoding_files:\n",
    "        data = np.load(EV_DECODING_DIR + this_file, allow_pickle = True).item()\n",
    "        decoding_results[region_y].append(data[region_y]['accuracy'])\n",
    "        decoding_results[fit_type].append(data[fit_type]['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777d5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize = (4,1.5), sharey = True, sharex = True)\n",
    "\n",
    "xaxs = np.arange(0, 1.5, 0.005)\n",
    "\n",
    "tstart = 0\n",
    "tend = 165\n",
    "for k, fit_type in enumerate(list(optimal_rank_dict)):\n",
    "    region_x, region_y = fit_type.split('_')\n",
    "    x = []\n",
    "    for i in range(len(decoding_results[region_x])):\n",
    "        x.append(np.clip(decoding_results[fit_type][i][tstart:tend]/decoding_results[region_x][i][tstart:tend],0,1))\n",
    "    npts = np.sum(~np.isnan(x), axis =0)\n",
    "    vals_mean = np.nanmean(x, axis = 0)\n",
    "    vals_sem = np.nanstd(x, axis = 0)/np.sqrt(npts)\n",
    "    \n",
    "    axs[k].plot(xaxs[tstart:tend],\n",
    "                vals_mean,\n",
    "                c = SPEC.COLS[region_x],\n",
    "                ls = '-')\n",
    "    axs[k].fill_between(xaxs[tstart:tend],\n",
    "                    vals_mean - vals_sem, \n",
    "                    vals_mean + vals_sem,\n",
    "                    color = SPEC.COLS[region_x],\n",
    "                    alpha = 0.3)\n",
    "    print(np.mean(vals_mean))\n",
    "    axs[k].set_ylim([-0.,0.85])\n",
    "    axs[k].set_xticks(np.arange(0, 1.0, 0.2))\n",
    "    axs[k].set_xlabel('Time from clicks on [s]')\n",
    "    axs[k].set_title(f\"{region_x} $\\\\rightarrow$ {region_y} vs {region_x}\")\n",
    "\n",
    "axs[0].set_ylabel('Decoding performance \\n(CS/population)')\n",
    "\n",
    "sns.despine()\n",
    "savethisfig(SPEC.FIGUREDIR + \"figure2/\", 'figure2_CSdecoding')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
